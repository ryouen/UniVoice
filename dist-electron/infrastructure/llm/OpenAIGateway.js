"use strict";
/**
 * OpenAI Gateway å®Ÿè£…
 *
 * âš ï¸ é‡è¦:
 * - GPT-5ã‚·ãƒªãƒ¼ã‚ºã¨Responses APIã¯å®Ÿåœ¨ã™ã‚‹ï¼ˆ2025å¹´8æœˆç¾åœ¨ï¼‰
 * - æ—¢å­˜ã®å‹•ä½œç¢ºèªæ¸ˆã¿å®Ÿè£…ã‚’å¤‰æ›´ã—ãªã„
 * - Shadow Modeå¯¾å¿œã§å®‰å…¨ãªç§»è¡Œã‚’å®Ÿç¾
 */
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.OpenAIGateway = void 0;
const openai_1 = __importDefault(require("openai"));
const types_1 = require("./types");
const logger_1 = require("../../utils/logger");
/**
 * OpenAI Responses API å®Ÿè£…
 * æ—¢å­˜ã®UnifiedPipelineServiceã¨äº’æ›æ€§ã‚’ä¿ã¤
 */
class OpenAIGateway {
    constructor(config) {
        this.componentLogger = logger_1.logger.child('OpenAIGateway');
        this.lastMetrics = null;
        // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ãå¯èƒ½ï¼‰
        this.DEFAULT_MAX_TOKENS = {
            [types_1.LLMPurpose.TRANSLATION]: 1500,
            [types_1.LLMPurpose.SUMMARY]: 1500,
            [types_1.LLMPurpose.SUMMARY_TRANSLATE]: 1500,
            [types_1.LLMPurpose.USER_TRANSLATE]: 1500,
            [types_1.LLMPurpose.VOCABULARY]: 1500,
            [types_1.LLMPurpose.REPORT]: 8192
        };
        this.config = config;
        this.openai = new openai_1.default({ apiKey: config.apiKey });
        // èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’æ¤œè¨¼
        this.validateModels();
    }
    /**
     * ãƒ¢ãƒ‡ãƒ«è¨­å®šã®æ¤œè¨¼
     * GPT-5ã‚·ãƒªãƒ¼ã‚ºä»¥å¤–ã®ä½¿ç”¨ã‚’é˜²ã
     */
    validateModels() {
        const validModels = ['gpt-5-nano', 'gpt-5-mini', 'gpt-5'];
        const configuredModels = Object.values(this.config.models);
        configuredModels.forEach(model => {
            if (!validModels.includes(model)) {
                const error = new Error(`âŒ ç„¡åŠ¹ãªãƒ¢ãƒ‡ãƒ«è¨­å®š: ${model}\n` +
                    `âœ… æœ‰åŠ¹ãªãƒ¢ãƒ‡ãƒ«: ${validModels.join(', ')}\n` +
                    `GPT-5ã‚·ãƒªãƒ¼ã‚ºã¯å®Ÿåœ¨ã—ã¾ã™ï¼ˆ2025å¹´8æœˆç¾åœ¨ï¼‰ã€‚GPT-4ã¸ã®å¤‰æ›´ã¯ç¦æ­¢ã§ã™ã€‚`);
                this.componentLogger.error('Model validation failed', { model, validModels });
                throw error;
            }
        });
        this.componentLogger.info('Model configuration validated', {
            models: this.config.models
        });
    }
    /**
     * åŒæœŸçš„ãªLLMå‘¼ã³å‡ºã—
     * æ—¢å­˜ã®executeTranslationã¨äº’æ›æ€§ã‚’ä¿ã¤
     */
    async complete(request) {
        const startTime = Date.now();
        const requestId = `req-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
        const model = this.config.models[request.purpose];
        const maxTokens = this.config.maxTokens?.[request.purpose] || this.DEFAULT_MAX_TOKENS[request.purpose];
        try {
            // âœ… æ—¢å­˜ã®å‹•ä½œç¢ºèªæ¸ˆã¿å®Ÿè£…ã¨åŒã˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            const response = await this.openai.responses.create({
                model,
                input: [
                    { role: 'system', content: request.systemPrompt },
                    { role: 'user', content: request.userContent }
                ],
                max_output_tokens: request.maxTokens || maxTokens,
                reasoning: {
                    effort: this.getReasoningEffort(request.purpose)
                },
                temperature: 1.0, // GPT-5ã§ã¯å›ºå®š
                stream: false
            });
            const latencyMs = Date.now() - startTime;
            // ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ï¼ˆShadow Modeç”¨ï¼‰
            this.lastMetrics = {
                requestId,
                purpose: request.purpose,
                model,
                latencyMs,
                tokenCount: response.usage?.total_tokens || 0,
                success: true
            };
            this.componentLogger.info('LLM request completed', {
                requestId,
                purpose: request.purpose,
                model,
                latencyMs,
                tokenCount: response.usage?.total_tokens
            });
            return {
                content: response.output_text,
                usage: response.usage ? {
                    promptTokens: response.usage.prompt_tokens || 0, // ğŸ”´ å…ƒã®å€¤ã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼ˆå‹ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
                    completionTokens: response.usage.completion_tokens || 0, // ğŸ”´ å…ƒã®å€¤ã‚’ãã®ã¾ã¾ä½¿ç”¨  
                    totalTokens: response.usage.total_tokens || 0
                } : undefined,
                metadata: {
                    model,
                    latencyMs,
                    requestId
                }
            };
        }
        catch (error) {
            const errorCode = this.getErrorCode(error);
            // ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰
            this.lastMetrics = {
                requestId,
                purpose: request.purpose,
                model,
                latencyMs: Date.now() - startTime,
                tokenCount: 0,
                success: false,
                errorCode
            };
            this.componentLogger.error('LLM request failed', {
                error,
                requestId,
                purpose: request.purpose,
                model
            });
            throw this.wrapError(error);
        }
    }
    /**
     * ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°LLMå‘¼ã³å‡ºã—
     * æ—¢å­˜ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç¿»è¨³ã¨å®Œå…¨äº’æ›
     */
    async *stream(request) {
        const startTime = Date.now();
        const requestId = `req-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
        const model = this.config.models[request.purpose];
        const maxTokens = this.config.maxTokens?.[request.purpose] || this.DEFAULT_MAX_TOKENS[request.purpose];
        try {
            // âœ… æ—¢å­˜ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè£…ã¨åŒã˜
            const stream = await this.openai.responses.create({
                model,
                input: [
                    { role: 'system', content: request.systemPrompt },
                    { role: 'user', content: request.userContent }
                ],
                max_output_tokens: request.maxTokens || maxTokens,
                reasoning: {
                    effort: this.getReasoningEffort(request.purpose)
                },
                temperature: 1.0,
                stream: true // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æœ‰åŠ¹
            });
            let tokenCount = 0;
            for await (const chunk of stream) {
                // æ—¢å­˜å®Ÿè£…ã¨åŒã˜chunk.typeåˆ¤å®š
                if (chunk.type === 'response.output_text.delta' && chunk.delta) {
                    tokenCount++;
                    yield {
                        delta: chunk.delta,
                        isComplete: false
                    };
                }
            }
            // å®Œäº†ã‚·ã‚°ãƒŠãƒ«
            yield {
                delta: '',
                isComplete: true
            };
            // ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
            this.lastMetrics = {
                requestId,
                purpose: request.purpose,
                model,
                latencyMs: Date.now() - startTime,
                tokenCount,
                success: true
            };
            this.componentLogger.info('LLM streaming completed', {
                requestId,
                purpose: request.purpose,
                model,
                latencyMs: Date.now() - startTime,
                tokenCount
            });
        }
        catch (error) {
            const errorCode = this.getErrorCode(error);
            this.lastMetrics = {
                requestId,
                purpose: request.purpose,
                model,
                latencyMs: Date.now() - startTime,
                tokenCount: 0,
                success: false,
                errorCode
            };
            this.componentLogger.error('LLM streaming failed', {
                error,
                requestId,
                purpose: request.purpose,
                model
            });
            throw this.wrapError(error);
        }
    }
    /**
     * ãƒ¢ãƒ‡ãƒ«åå–å¾—
     */
    getModelForPurpose(purpose) {
        return this.config.models[purpose];
    }
    /**
     * ãƒ¬ãƒ¼ãƒˆåˆ¶é™çŠ¶æ…‹ï¼ˆå°†æ¥å®Ÿè£…ï¼‰
     */
    getRateLimitStatus() {
        // TODO: å®Ÿéš›ã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™æƒ…å ±ã‚’å®Ÿè£…
        return {
            remaining: 1000,
            resetAt: new Date(Date.now() + 3600000)
        };
    }
    /**
     * ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
     */
    async isHealthy() {
        try {
            // æœ€å°é™ã®ãƒ†ã‚¹ãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            await this.complete({
                purpose: types_1.LLMPurpose.TRANSLATION,
                systemPrompt: 'Health check',
                userContent: 'Test',
                maxTokens: 1
            });
            return true;
        }
        catch {
            return false;
        }
    }
    /**
     * æœ€å¾Œã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—ï¼ˆShadow Modeç”¨ï¼‰
     */
    getLastMetrics() {
        return this.lastMetrics;
    }
    /**
     * ç”¨é€”ã«å¿œã˜ãŸæ¨è«–åŠªåŠ›ãƒ¬ãƒ™ãƒ«
     * æ—¢å­˜å®Ÿè£…ã¨åŒã˜è¨­å®š
     */
    getReasoningEffort(purpose) {
        switch (purpose) {
            case types_1.LLMPurpose.REPORT:
                return 'high';
            case types_1.LLMPurpose.SUMMARY:
            case types_1.LLMPurpose.VOCABULARY:
                return 'low';
            case types_1.LLMPurpose.TRANSLATION:
            case types_1.LLMPurpose.SUMMARY_TRANSLATE:
            case types_1.LLMPurpose.USER_TRANSLATE:
            default:
                return 'minimal';
        }
    }
    /**
     * ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰åˆ¤å®š
     */
    getErrorCode(error) {
        if (error instanceof Error) {
            const message = error.message.toLowerCase();
            if (message.includes('rate_limit') || message.includes('429')) {
                return 'RATE_LIMIT_ERROR';
            }
            if (message.includes('unauthorized') || message.includes('401')) {
                return 'AUTH_ERROR';
            }
            if (message.includes('timeout')) {
                return 'TIMEOUT_ERROR';
            }
        }
        return 'UNKNOWN_ERROR';
    }
    /**
     * ã‚¨ãƒ©ãƒ¼ãƒ©ãƒƒãƒ”ãƒ³ã‚°
     */
    wrapError(error) {
        const errorCode = this.getErrorCode(error);
        const isRetryable = ['RATE_LIMIT_ERROR', 'TIMEOUT_ERROR'].includes(errorCode);
        if (error instanceof Error) {
            return Object.assign(error, {
                code: errorCode,
                retryable: isRetryable,
                details: error
            });
        }
        const wrappedError = new Error(String(error));
        wrappedError.code = errorCode;
        wrappedError.retryable = isRetryable;
        return wrappedError;
    }
}
exports.OpenAIGateway = OpenAIGateway;
