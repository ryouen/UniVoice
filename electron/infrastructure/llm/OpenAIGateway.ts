/**
 * OpenAI Gateway å®Ÿè£…
 * 
 * âš ï¸ é‡è¦: 
 * - GPT-5ã‚·ãƒªãƒ¼ã‚ºã¨Responses APIã¯å®Ÿåœ¨ã™ã‚‹ï¼ˆ2025å¹´8æœˆç¾åœ¨ï¼‰
 * - æ—¢å­˜ã®å‹•ä½œç¢ºèªæ¸ˆã¿å®Ÿè£…ã‚’å¤‰æ›´ã—ãªã„
 * - Shadow Modeå¯¾å¿œã§å®‰å…¨ãªç§»è¡Œã‚’å®Ÿç¾
 */

import OpenAI from 'openai';
import { 
  LLMGateway, 
  LLMRequest, 
  LLMResponse, 
  LLMStreamChunk,
  LLMPurpose,
  LLMConfig,
  LLMMetrics,
  LLMError
} from './types';
import { logger } from '../../utils/logger';

/**
 * OpenAI Responses API å®Ÿè£…
 * æ—¢å­˜ã®UnifiedPipelineServiceã¨äº’æ›æ€§ã‚’ä¿ã¤
 */
export class OpenAIGateway implements LLMGateway {
  private openai: OpenAI;
  private config: LLMConfig;
  private componentLogger = logger.child('OpenAIGateway');
  private lastMetrics: LLMMetrics | null = null;

  // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ãå¯èƒ½ï¼‰
  private readonly DEFAULT_MAX_TOKENS = {
    [LLMPurpose.TRANSLATION]: 1500,
    [LLMPurpose.SUMMARY]: 1500,
    [LLMPurpose.SUMMARY_TRANSLATE]: 1500,
    [LLMPurpose.USER_TRANSLATE]: 1500,
    [LLMPurpose.VOCABULARY]: 1500,
    [LLMPurpose.REPORT]: 8192
  };

  constructor(config: LLMConfig) {
    this.config = config;
    this.openai = new OpenAI({ apiKey: config.apiKey });
    
    // èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’æ¤œè¨¼
    this.validateModels();
  }

  /**
   * ãƒ¢ãƒ‡ãƒ«è¨­å®šã®æ¤œè¨¼
   * GPT-5ã‚·ãƒªãƒ¼ã‚ºä»¥å¤–ã®ä½¿ç”¨ã‚’é˜²ã
   */
  private validateModels(): void {
    const validModels = ['gpt-5-nano', 'gpt-5-mini', 'gpt-5'];
    const configuredModels = Object.values(this.config.models);
    
    configuredModels.forEach(model => {
      if (!validModels.includes(model)) {
        const error = new Error(
          `âŒ ç„¡åŠ¹ãªãƒ¢ãƒ‡ãƒ«è¨­å®š: ${model}\n` +
          `âœ… æœ‰åŠ¹ãªãƒ¢ãƒ‡ãƒ«: ${validModels.join(', ')}\n` +
          `GPT-5ã‚·ãƒªãƒ¼ã‚ºã¯å®Ÿåœ¨ã—ã¾ã™ï¼ˆ2025å¹´8æœˆç¾åœ¨ï¼‰ã€‚GPT-4ã¸ã®å¤‰æ›´ã¯ç¦æ­¢ã§ã™ã€‚`
        );
        this.componentLogger.error('Model validation failed', { model, validModels });
        throw error;
      }
    });
    
    this.componentLogger.info('Model configuration validated', { 
      models: this.config.models 
    });
  }

  /**
   * åŒæœŸçš„ãªLLMå‘¼ã³å‡ºã—
   * æ—¢å­˜ã®executeTranslationã¨äº’æ›æ€§ã‚’ä¿ã¤
   */
  async complete(request: LLMRequest): Promise<LLMResponse> {
    const startTime = Date.now();
    const requestId = `req-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
    const model = this.config.models[request.purpose];
    const maxTokens = this.config.maxTokens?.[request.purpose] || this.DEFAULT_MAX_TOKENS[request.purpose];
    
    try {
      // âœ… æ—¢å­˜ã®å‹•ä½œç¢ºèªæ¸ˆã¿å®Ÿè£…ã¨åŒã˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
      const response = await this.openai.responses.create({
        model,
        input: [
          { role: 'system', content: request.systemPrompt },
          { role: 'user', content: request.userContent }
        ],
        max_output_tokens: request.maxTokens || maxTokens,
        reasoning: { 
          effort: this.getReasoningEffort(request.purpose) 
        },
        temperature: 1.0, // GPT-5ã§ã¯å›ºå®š
        stream: false
      });

      const latencyMs = Date.now() - startTime;

      // ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ï¼ˆShadow Modeç”¨ï¼‰
      this.lastMetrics = {
        requestId,
        purpose: request.purpose,
        model,
        latencyMs,
        tokenCount: response.usage?.total_tokens || 0,
        success: true
      };

      this.componentLogger.info('LLM request completed', {
        requestId,
        purpose: request.purpose,
        model,
        latencyMs,
        tokenCount: response.usage?.total_tokens
      });

      return {
        content: response.output_text,
        usage: response.usage ? {
          promptTokens: (response.usage as any).prompt_tokens || 0,  // ğŸ”´ å…ƒã®å€¤ã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼ˆå‹ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
          completionTokens: (response.usage as any).completion_tokens || 0,  // ğŸ”´ å…ƒã®å€¤ã‚’ãã®ã¾ã¾ä½¿ç”¨  
          totalTokens: response.usage.total_tokens || 0
        } : undefined,
        metadata: {
          model,
          latencyMs,
          requestId
        }
      };
    } catch (error) {
      const errorCode = this.getErrorCode(error);
      
      // ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰
      this.lastMetrics = {
        requestId,
        purpose: request.purpose,
        model,
        latencyMs: Date.now() - startTime,
        tokenCount: 0,
        success: false,
        errorCode
      };

      this.componentLogger.error('LLM request failed', { 
        error,
        requestId,
        purpose: request.purpose,
        model
      });
      
      throw this.wrapError(error);
    }
  }

  /**
   * ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°LLMå‘¼ã³å‡ºã—
   * æ—¢å­˜ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç¿»è¨³ã¨å®Œå…¨äº’æ›
   */
  async *stream(request: LLMRequest): AsyncIterableIterator<LLMStreamChunk> {
    const startTime = Date.now();
    const requestId = `req-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
    const model = this.config.models[request.purpose];
    const maxTokens = this.config.maxTokens?.[request.purpose] || this.DEFAULT_MAX_TOKENS[request.purpose];

    try {
      // âœ… æ—¢å­˜ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè£…ã¨åŒã˜
      const stream = await this.openai.responses.create({
        model,
        input: [
          { role: 'system', content: request.systemPrompt },
          { role: 'user', content: request.userContent }
        ],
        max_output_tokens: request.maxTokens || maxTokens,
        reasoning: { 
          effort: this.getReasoningEffort(request.purpose) 
        },
        temperature: 1.0,
        stream: true // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æœ‰åŠ¹
      });

      let tokenCount = 0;

      for await (const chunk of stream) {
        // æ—¢å­˜å®Ÿè£…ã¨åŒã˜chunk.typeåˆ¤å®š
        if (chunk.type === 'response.output_text.delta' && chunk.delta) {
          tokenCount++;
          yield {
            delta: chunk.delta,
            isComplete: false
          };
        }
      }

      // å®Œäº†ã‚·ã‚°ãƒŠãƒ«
      yield { 
        delta: '', 
        isComplete: true 
      };

      // ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
      this.lastMetrics = {
        requestId,
        purpose: request.purpose,
        model,
        latencyMs: Date.now() - startTime,
        tokenCount,
        success: true
      };

      this.componentLogger.info('LLM streaming completed', {
        requestId,
        purpose: request.purpose,
        model,
        latencyMs: Date.now() - startTime,
        tokenCount
      });

    } catch (error) {
      const errorCode = this.getErrorCode(error);
      
      this.lastMetrics = {
        requestId,
        purpose: request.purpose,
        model,
        latencyMs: Date.now() - startTime,
        tokenCount: 0,
        success: false,
        errorCode
      };

      this.componentLogger.error('LLM streaming failed', { 
        error,
        requestId,
        purpose: request.purpose,
        model
      });
      
      throw this.wrapError(error);
    }
  }

  /**
   * ãƒ¢ãƒ‡ãƒ«åå–å¾—
   */
  getModelForPurpose(purpose: LLMPurpose): string {
    return this.config.models[purpose];
  }

  /**
   * ãƒ¬ãƒ¼ãƒˆåˆ¶é™çŠ¶æ…‹ï¼ˆå°†æ¥å®Ÿè£…ï¼‰
   */
  getRateLimitStatus(): { remaining: number; resetAt: Date } {
    // TODO: å®Ÿéš›ã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™æƒ…å ±ã‚’å®Ÿè£…
    return {
      remaining: 1000,
      resetAt: new Date(Date.now() + 3600000)
    };
  }

  /**
   * ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
   */
  async isHealthy(): Promise<boolean> {
    try {
      // æœ€å°é™ã®ãƒ†ã‚¹ãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ
      await this.complete({
        purpose: LLMPurpose.TRANSLATION,
        systemPrompt: 'Health check',
        userContent: 'Test',
        maxTokens: 1
      });
      return true;
    } catch {
      return false;
    }
  }

  /**
   * æœ€å¾Œã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—ï¼ˆShadow Modeç”¨ï¼‰
   */
  getLastMetrics(): LLMMetrics | null {
    return this.lastMetrics;
  }

  /**
   * ç”¨é€”ã«å¿œã˜ãŸæ¨è«–åŠªåŠ›ãƒ¬ãƒ™ãƒ«
   * æ—¢å­˜å®Ÿè£…ã¨åŒã˜è¨­å®š
   */
  private getReasoningEffort(purpose: LLMPurpose): 'minimal' | 'low' | 'medium' | 'high' {
    switch (purpose) {
      case LLMPurpose.REPORT:
        return 'high';
      case LLMPurpose.SUMMARY:
      case LLMPurpose.VOCABULARY:
        return 'low';
      case LLMPurpose.TRANSLATION:
      case LLMPurpose.SUMMARY_TRANSLATE:
      case LLMPurpose.USER_TRANSLATE:
      default:
        return 'minimal';
    }
  }

  /**
   * ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰åˆ¤å®š
   */
  private getErrorCode(error: unknown): string {
    if (error instanceof Error) {
      const message = error.message.toLowerCase();
      if (message.includes('rate_limit') || message.includes('429')) {
        return 'RATE_LIMIT_ERROR';
      }
      if (message.includes('unauthorized') || message.includes('401')) {
        return 'AUTH_ERROR';
      }
      if (message.includes('timeout')) {
        return 'TIMEOUT_ERROR';
      }
    }
    return 'UNKNOWN_ERROR';
  }

  /**
   * ã‚¨ãƒ©ãƒ¼ãƒ©ãƒƒãƒ”ãƒ³ã‚°
   */
  private wrapError(error: unknown): LLMError {
    const errorCode = this.getErrorCode(error);
    const isRetryable = ['RATE_LIMIT_ERROR', 'TIMEOUT_ERROR'].includes(errorCode);

    if (error instanceof Error) {
      return Object.assign(error, {
        code: errorCode,
        retryable: isRetryable,
        details: error
      });
    }

    const wrappedError = new Error(String(error)) as LLMError;
    wrappedError.code = errorCode;
    wrappedError.retryable = isRetryable;
    return wrappedError;
  }
}