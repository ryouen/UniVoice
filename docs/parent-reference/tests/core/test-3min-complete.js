#!/usr/bin/env node
/**
 * test-3min-complete.js
 * 
 * test-20min-production-detailed.jsã®3åˆ†ç‰ˆï¼ˆåŒä¸€å®Ÿè£…ï¼‰
 * TEST_NAME='3min-complete', TEST_DURATION_MS=180ç§’ã®ã¿ç•°ãªã‚‹
 * 
 * â€» æœ¬ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°æ™‚ã¯CLAUDE.mdã®è©²å½“ç®‡æ‰€ã‚’æ›´æ–°ã™ã‚‹ã“ã¨
 */

require('dotenv').config();
const fs = require('fs');
const path = require('path');
const WebSocket = require('ws');
const { performance } = require('perf_hooks');

// ========== è¨­å®š ==========
const TEST_NAME = '3min-complete';
const TEST_DURATION_MS = 180 * 1000;  // 3åˆ†
const AUDIO_FILE = './sample_voice/Hayes.wav';
const LOOP_AUDIO = false;

// ãƒ•ãƒ¬ãƒ¼ãƒ è¨­å®š
const FRAME_MS = 20;
const FRAME_SIZE = 640;
const FRAMES_PER_SECOND = 50;

// Deepgram params
const DG_ENDPOINTING = (process.env.DG_ENDPOINTING || '800').toString();
const DG_UTTERANCE_END_MS = (process.env.DG_UTTERANCE_END_MS || '1000').toString();
const DG_INTERIM = (process.env.DG_INTERIM || 'true').toString();
const DG_DIARIZE = (process.env.DG_DIARIZE || 'false').toString();
const DG_MODEL = (process.env.DG_MODEL || 'nova-3').toString();

const DG_PARAMS = new URLSearchParams({
  encoding: 'linear16',
  sample_rate: '16000',
  interim_results: DG_INTERIM,
  endpointing: DG_ENDPOINTING,
  utterance_end_ms: DG_UTTERANCE_END_MS,
  punctuate: 'true',
  diarize: DG_DIARIZE,
  model: DG_MODEL,
  version: 'latest'
});

const DEEPGRAM_WS_URL = `wss://api.deepgram.com/v1/listen?${DG_PARAMS}`;

// OpenAIè¨­å®š
const OpenAI = require('openai');
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// â‘¢â‘£â‘¤â‘¥ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
const {
  generateSummary,
  translateSummary,
  generateVocabulary,
  generateFinalReport
} = require('../helpers/gpt5-helpers.js');

// å‹•çš„è¨­å®š
let MAX_TOKENS = parseInt(process.env.OPENAI_TRANSLATE_MAX_TOKENS || '1500', 10);
let CONCURRENCY = parseInt(process.env.OPENAI_TRANSLATE_CONCURRENCY || '3', 10);
let CURRENT_MODEL = process.env.OPENAI_MODEL_TRANSLATE || 'gpt-5-nano';
let INTERIM_THROTTLE_MS = 500;

// å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
const OUT_DIR = path.resolve('test-results');
if (!fs.existsSync(OUT_DIR)) fs.mkdirSync(OUT_DIR, { recursive: true });

const RUN_ID = new Date().toISOString().replace(/[:.]/g, '-');
const LOG_FILE = path.join(OUT_DIR, `${TEST_NAME}_log_${RUN_ID}.jsonl`);
const METRICS_FILE = path.join(OUT_DIR, `${TEST_NAME}_metrics_${RUN_ID}.json`);
const REPORT_FILE = path.join(OUT_DIR, `${TEST_NAME}_report_${RUN_ID}.md`);
const DETAILED_FILE = path.join(OUT_DIR, `${TEST_NAME}_detailed_${RUN_ID}.json`);

// ========== è©³ç´°è¨˜éŒ²ç”¨ãƒ‡ãƒ¼ã‚¿æ§‹é€  ==========
const detailedData = {
  testInfo: {
    runId: RUN_ID,
    startTime: new Date().toISOString(),
    audioFile: AUDIO_FILE,
    models: {
      asr: DG_MODEL,
      translation: CURRENT_MODEL
    },
    params: {
      maxTokens: MAX_TOKENS,
      concurrency: CONCURRENCY,
      interimThrottle: INTERIM_THROTTLE_MS
    }
  },
  
  // â‘ éŸ³å£°å…¥åŠ›
  stage1_audioInput: {
    totalFrames: 0,
    totalBytes: 0,
    frameSamples: [],  // æœ€åˆã¨æœ€å¾Œã®10ãƒ•ãƒ¬ãƒ¼ãƒ 
    timestamps: []
  },
  
  // â‘¡ASRï¼ˆéŸ³å£°èªè­˜ï¼‰
  stage2_asr: {
    interim: [],
    final: [],
    confidence: [],
    timeline: []  // åˆ†å˜ä½ã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³
  },
  
  // â‘¢â‘£â‘¤è¦ç´„ãƒ‡ãƒ¼ã‚¿
  summaries: [],
  
  // â‘¢ç¿»è¨³è¦æ±‚
  stage3_translationRequests: {
    requests: [],
    throttled: 0,
    queued: 0
  },
  
  // â‘£GPTå‡¦ç†
  stage4_gptProcessing: {
    requests: [],
    tokens: 0,
    cost: 0
  },
  
  // â‘¤å‡ºåŠ›æŠ½å‡º
  stage5_outputExtraction: {
    successful: [],
    empty: [],
    extractionDetails: []
  },
  
  // â‘¥è¡¨ç¤ºï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã«è¦‹ãˆã‚‹å…¨ãƒ‡ãƒ¼ã‚¿ï¼‰
  stage6_display: {
    translations: [],  // ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«è¡¨ç¤ºã•ã‚Œã‚‹å…¨ç¿»è¨³
    firstPaints: [],
    timeline: []  // æ™‚ç³»åˆ—è¡¨ç¤ºãƒ‡ãƒ¼ã‚¿
  },
  
  // è¦ç´„ãƒ»èªå½™
  summaries: [],
  vocabulary: [],
  finalReport: null
};

// ========== Translation queue ==========
let ACTIVE = 0;
const QUEUE = [];

function enqueueTranslate(fn) {
  QUEUE.push(fn);
  drainQueue();
}

function drainQueue() {
  while (ACTIVE < CONCURRENCY && QUEUE.length > 0) {
    const fn = QUEUE.shift();
    ACTIVE++;
    Promise.resolve()
      .then(fn)
      .catch(() => {})
      .finally(() => {
        ACTIVE--;
        drainQueue();
      });
  }
}

// ========== ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ  ==========
const logStream = fs.createWriteStream(LOG_FILE, { flags: 'a' });

function log(stage, data) {
  const entry = {
    timestamp: Date.now(),
    stage,
    data
  };
  
  logStream.write(JSON.stringify(entry) + '\n');
  
  // é‡è¦ãªã‚¤ãƒ™ãƒ³ãƒˆã¯ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
  if (stage === 'error' || stage === 'stopLoss' || data.milestone) {
    console.log(`[${stage}] ${data.message || JSON.stringify(data).substring(0, 100)}`);
  }
}

// ========== ãƒ¡ãƒˆãƒªã‚¯ã‚¹ ==========
const metrics = {
  startTime: Date.now(),
  endTime: null,
  duration: null,
  translations: {
    attempted: 0,
    successful: 0,
    empty: 0,
    errors: 0
  },
  asr: {
    interim: 0,
    final: 0,
    confidence: []
  },
  performance: {
    firstPaints: [],
    latencies: []
  },
  errors: []
};

// ========== éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ ==========
let audioBuffer;
let frameIndex = 0;

function loadAudio() {
  try {
    const audioData = fs.readFileSync(AUDIO_FILE);
    audioBuffer = audioData.slice(44); // WAVãƒ˜ãƒƒãƒ€ãƒ¼ã‚¹ã‚­ãƒƒãƒ—
    const totalFrames = Math.floor(audioBuffer.length / FRAME_SIZE);
    
    detailedData.stage1_audioInput.totalFrames = totalFrames;
    detailedData.stage1_audioInput.totalBytes = audioBuffer.length;
    
    console.log(`âœ… Audio loaded: ${audioBuffer.length} bytes, ${totalFrames} frames`);
    console.log(`   Duration: ${(totalFrames * FRAME_MS / 1000).toFixed(1)} seconds`);
    
    return true;
  } catch (error) {
    console.error('âŒ Failed to load audio:', error.message);
    return false;
  }
}

// ========== Deepgram WebSocket ==========
let ws;
let lastInterimText = '';
let lastInterimTime = 0;
let currentMinute = 0;
let minuteTranscript = '';

// è¦ç´„ç”¨ã®ãƒãƒƒãƒ•ã‚¡ã¨åˆ¶å¾¡å¤‰æ•°
let transcriptBuffer = [];
let lastSummaryMinute = -1;
const SUMMARY_INTERVAL_MINUTES = 2;  // 3åˆ†ãƒ†ã‚¹ãƒˆç”¨ã«2åˆ†ã«å¤‰æ›´

function connectDeepgram() {
  ws = new WebSocket(DEEPGRAM_WS_URL, {
    headers: { 'Authorization': `Token ${process.env.DEEPGRAM_API_KEY}` }
  });

  ws.on('open', () => {
    console.log('ğŸ”Œ Deepgram connected');
    log('websocket', { status: 'connected' });
    startAudioStream();
  });

  ws.on('message', (data) => {
    try {
      const msg = JSON.parse(data.toString());
      
      if (msg.type === 'Results') {
        const alt = msg.channel?.alternatives?.[0];
        if (!alt) return;
        
        const text = alt.transcript;
        const confidence = alt.confidence || 0;
        const isFinal = msg.is_final;
        
        if (!text) return;
        
        const timestamp = Date.now();
        const minute = Math.floor((timestamp - metrics.startTime) / 60000);
        
        // â‘¡ASRè¨˜éŒ²
        const asrRecord = {
          timestamp,
          minute,
          text,
          confidence,
          isFinal
        };
        
        if (isFinal) {
          // Finalçµæœ
          detailedData.stage2_asr.final.push(asrRecord);
          metrics.asr.final++;
          metrics.asr.confidence.push(confidence);
          
          log('asr_final', { text, confidence, segment_id: metrics.asr.final });
          
          // ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³æ›´æ–°
          minuteTranscript += text + ' ';
          
          // è»¢å†™ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
          transcriptBuffer.push(text);
          
          // â‘¢ç¿»è¨³è¦æ±‚ä½œæˆ
          const requestId = `req_${RUN_ID}_${metrics.translations.attempted++}`;
          const translationRequest = {
            requestId,
            timestamp,
            text,
            confidence,
            type: 'final'
          };
          
          detailedData.stage3_translationRequests.requests.push(translationRequest);
          
          // ç¿»è¨³å®Ÿè¡Œ
          enqueueTranslate(async () => {
            await processTranslation(text, confidence, requestId, false);
          });
          
        } else {
          // Interimçµæœ
          detailedData.stage2_asr.interim.push(asrRecord);
          metrics.asr.interim++;
          
          // ã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°
          const now = Date.now();
          if (now - lastInterimTime < INTERIM_THROTTLE_MS) {
            detailedData.stage3_translationRequests.throttled++;
            return;
          }
          
          if (text !== lastInterimText && text.length > 8) {
            lastInterimText = text;
            lastInterimTime = now;
            
            const requestId = `req_${RUN_ID}_interim_${metrics.asr.interim}`;
            
            enqueueTranslate(async () => {
              await processTranslation(text, confidence, requestId, true);
            });
          }
        }
        
        // åˆ†å˜ä½ã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³æ›´æ–°
        if (minute > currentMinute) {
          if (minuteTranscript) {
            detailedData.stage2_asr.timeline.push({
              minute: currentMinute,
              timeRange: formatTimeRange(currentMinute),
              transcript: minuteTranscript.trim(),
              wordCount: minuteTranscript.split(' ').length
            });
          }
          currentMinute = minute;
          minuteTranscript = '';
          
          // 10åˆ†ã”ã¨ã®è¦ç´„å‡¦ç†
          if (minute >= SUMMARY_INTERVAL_MINUTES && 
              minute % SUMMARY_INTERVAL_MINUTES === 0 && 
              minute !== lastSummaryMinute) {
            
            lastSummaryMinute = minute;
            
            // éå»10åˆ†ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
            const last10MinText = transcriptBuffer.join(' ');
            transcriptBuffer = []; // ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢
            
            console.log(`\nğŸ”„ Processing 10-minute summary at minute ${minute}...`);
            
            // éåŒæœŸå‡¦ç†ï¼ˆPromiseã§å®Ÿè¡Œï¼‰
            (async () => {
              try {
                // â‘¢å®šæœŸè¦ç´„ã‚’ç”Ÿæˆ
                const summaryStartTime = Date.now();
                const englishSummary = await generateSummary(last10MinText);
                const summaryTime = Date.now() - summaryStartTime;
                
                console.log(`âœ… Summary generated in ${summaryTime}ms`);
                
                // â‘£è¦ç´„ç¿»è¨³ã¨â‘¤èªå½™æŠ½å‡ºã‚’ä¸¦åˆ—å®Ÿè¡Œ
                const [japaneseSummary, vocabulary] = await Promise.all([
                  translateSummary(englishSummary),
                  generateVocabulary(last10MinText)
                ]);
                
                // ãƒ‡ãƒ¼ã‚¿è¨˜éŒ²
                detailedData.summaries.push({
                  minute,
                  timeRange: formatTimeRange(minute - 10) + ' - ' + formatTimeRange(minute),
                  englishSummary,
                  japaneseSummary,
                  vocabulary,
                  wordCount: last10MinText.split(' ').length
                });
                
                console.log(`ğŸ“ Summary: "${englishSummary.substring(0, 100)}..."`);
                console.log(`ğŸ‡¯ğŸ‡µ Translation: "${japaneseSummary.substring(0, 100)}..."`);
                console.log(`ğŸ“š Vocabulary: ${vocabulary.length} terms extracted`);
                
              } catch (error) {
                console.error(`âŒ Summary processing failed:`, error.message);
                log('error', { 
                  stage: 'summary', 
                  minute, 
                  error: error.message 
                });
              }
            })();
          }
        }
      }
    } catch (error) {
      log('error', { stage: 'asr', error: error.message });
    }
  });

  ws.on('error', (error) => {
    console.error('âŒ WebSocket error:', error.message);
    log('error', { stage: 'websocket', error: error.message });
    metrics.errors.push({ stage: 'websocket', error: error.message, timestamp: Date.now() });
  });

  ws.on('close', () => {
    console.log('ğŸ”Œ Deepgram disconnected');
    log('websocket', { status: 'disconnected' });
  });
}

// ========== éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚° ==========
function startAudioStream() {
  const interval = setInterval(() => {
    if (!ws || ws.readyState !== WebSocket.OPEN) {
      clearInterval(interval);
      return;
    }
    
    if (frameIndex >= Math.floor(audioBuffer.length / FRAME_SIZE)) {
      if (!LOOP_AUDIO) {
        clearInterval(interval);
        ws.close();
        endTest();
        return;
      }
      frameIndex = 0;
    }
    
    const frame = audioBuffer.slice(
      frameIndex * FRAME_SIZE,
      (frameIndex + 1) * FRAME_SIZE
    );
    
    ws.send(frame);
    
    // â‘ éŸ³å£°å…¥åŠ›è¨˜éŒ²ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
    if (frameIndex < 10 || frameIndex === Math.floor(audioBuffer.length / FRAME_SIZE) - 1 || frameIndex % 1000 === 0) {
      detailedData.stage1_audioInput.frameSamples.push({
        frameIndex,
        timestamp: Date.now(),
        size: frame.length
      });
    }
    
    frameIndex++;
  }, FRAME_MS);
  
  // ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
  setTimeout(() => {
    clearInterval(interval);
    if (ws && ws.readyState === WebSocket.OPEN) {
      ws.close();
    }
    endTest();
  }, TEST_DURATION_MS);
}

// ========== ç¿»è¨³å‡¦ç†ï¼ˆtest-20min-productionã¨åŒã˜ï¼‰ ==========
async function processTranslation(text, confidence, requestId, isInterim = false) {
  const startTime = performance.now();
  
  // â‘£GPTå‡¦ç†è¨˜éŒ²
  const gptRequest = {
    requestId,
    timestamp: Date.now(),
    model: CURRENT_MODEL,
    text,
    isInterim,
    inputPrompt: `Translate English to Japanese. Output only the translation.\n\n${text}`
  };
  
  detailedData.stage4_gptProcessing.requests.push(gptRequest);
  
  let out = '', firstTokenMs = null, usage = null, status = 'completed';
  
  for (let attempt = 0; attempt < 3; attempt++) {
    try {
      const stream = await openai.responses.stream({
        model: CURRENT_MODEL,
        input: [
          { role: 'system', content: 'Translate English to Japanese. Output only the translation.' },
          { role: 'user', content: text }
        ],
        max_output_tokens: MAX_TOKENS,
        reasoning: { effort: 'minimal' },
        text: { verbosity: (process.env.OPENAI_TRANSLATE_VERBOSITY || 'low') }
      });
      
      for await (const chunk of stream) {
        if (chunk.type === 'response.output_text.delta' && chunk.delta) {
          if (!firstTokenMs) {
            firstTokenMs = performance.now() - startTime;
            
            // First paintè¨˜éŒ²
            detailedData.stage6_display.firstPaints.push({
              requestId,
              firstPaintMs: firstTokenMs,
              timestamp: Date.now()
            });
            
            log('first_paint', { requestId, firstPaintMs: firstTokenMs });
          }
          out += chunk.delta;
        } else if (chunk.type === 'response.done' && chunk.usage) {
          usage = chunk.usage;
        }
      }
      
      break; // æˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
      
    } catch (error) {
      console.error(`Translation attempt ${attempt + 1} failed:`, error.message);
      
      if (attempt === 2) {
        log('error', { 
          stage: 'translation', 
          requestId,
          error: error.message,
          category: 'FATAL'
        });
        metrics.translations.errors++;
        return;
      }
    }
  }
  
  const totalTime = performance.now() - startTime;
  const translation = out;
  
  // â‘¤å‡ºåŠ›æŠ½å‡ºè¨˜éŒ²
  const extractionResult = {
    requestId,
    timestamp: Date.now(),
    success: translation.length > 0,
    outputLength: translation.length,
    isEmpty: translation.trim().length === 0
  };
  
  detailedData.stage5_outputExtraction.extractionDetails.push(extractionResult);
  
  if (translation.trim()) {
    metrics.translations.successful++;
    detailedData.stage5_outputExtraction.successful.push(requestId);
    
    // â‘¥è¡¨ç¤ºè¨˜éŒ²ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã«è¦‹ãˆã‚‹å…¨ãƒ‡ãƒ¼ã‚¿ï¼‰
    const displayEntry = {
      timestamp: Date.now(),
      minute: Math.floor((Date.now() - metrics.startTime) / 60000),
      timeString: formatTime(Date.now() - metrics.startTime),
      original: text,
      translation,
      confidence,
      isInterim,
      firstPaintMs: firstTokenMs,
      totalTimeMs: totalTime,
      model: CURRENT_MODEL
    };
    
    detailedData.stage6_display.translations.push(displayEntry);
    
    log('translation_complete', displayEntry);
    
    // ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›ï¼ˆç°¡æ˜“è¡¨ç¤ºï¼‰
    const time = new Date().toISOString().split('T')[1].split('.')[0];
    console.log(`[${time}] ç¿»è¨³å®Œäº†: "${translation.substring(0, 50)}..." (${totalTime.toFixed(0)}ms)`);
    
  } else {
    metrics.translations.empty++;
    detailedData.stage5_outputExtraction.empty.push(requestId);
    log('translation_empty', { requestId });
  }
  
  // GPTå‡¦ç†ã®è©³ç´°æ›´æ–°
  gptRequest.response = translation;
  gptRequest.usage = usage;
  gptRequest.latencyMs = totalTime;
  gptRequest.firstPaintMs = firstTokenMs;
  gptRequest.status = status;
  
  // ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
  if (usage) {
    detailedData.stage4_gptProcessing.tokens += (usage.input_tokens || 0) + (usage.output_tokens || 0);
    detailedData.stage4_gptProcessing.cost += 
      ((usage.input_tokens || 0) * 0.00015 + (usage.output_tokens || 0) * 0.0006) / 1000;
  }
  
  metrics.performance.latencies.push(totalTime);
  if (firstTokenMs) {
    metrics.performance.firstPaints.push(firstTokenMs);
  }
}

// ========== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° ==========
function formatTime(ms) {
  const totalSeconds = Math.floor(ms / 1000);
  const minutes = Math.floor(totalSeconds / 60);
  const seconds = totalSeconds % 60;
  return `${String(minutes).padStart(2, '0')}:${String(seconds).padStart(2, '0')}`;
}

function formatTimeRange(minute) {
  const start = `${String(Math.floor(minute / 60)).padStart(2, '0')}:${String(minute % 60).padStart(2, '0')}`;
  const end = `${String(Math.floor((minute + 1) / 60)).padStart(2, '0')}:${String((minute + 1) % 60).padStart(2, '0')}`;
  return `${start}-${end}`;
}

// ========== ãƒ†ã‚¹ãƒˆçµ‚äº†å‡¦ç† ==========
async function endTest() {
  metrics.endTime = Date.now();
  metrics.duration = metrics.endTime - metrics.startTime;
  
  // æœ€å¾Œã®åˆ†ã®ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä¿å­˜
  if (minuteTranscript) {
    detailedData.stage2_asr.timeline.push({
      minute: currentMinute,
      timeRange: formatTimeRange(currentMinute),
      transcript: minuteTranscript.trim(),
      wordCount: minuteTranscript.split(' ').length
    });
  }
  
  // æœ€å¾Œã®è¦ç´„ã‚’å‡¦ç†ï¼ˆæ®‹ã‚Šã®ãƒãƒƒãƒ•ã‚¡ï¼‰
  if (transcriptBuffer.length > 0) {
    console.log('\nğŸ”„ Processing final summary...');
    try {
      const finalText = transcriptBuffer.join(' ');
      const englishSummary = await generateSummary(finalText);
      const [japaneseSummary, vocabulary] = await Promise.all([
        translateSummary(englishSummary),
        generateVocabulary(finalText)
      ]);
      
      detailedData.summaries.push({
        minute: Math.floor((Date.now() - metrics.startTime) / 60000),
        englishSummary,
        japaneseSummary,
        vocabulary
      });
    } catch (error) {
      console.error('Final summary failed:', error.message);
    }
  }
  
  // â‘¥AIæœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
  console.log('\nğŸ“Š Generating AI final report...');
  try {
    const reportStartTime = Date.now();
    
    // å…¨è»¢å†™ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
    const allTranscripts = detailedData.stage2_asr.final
      .map(item => item.text)
      .join(' ');
    
    // å…¨è¦ç´„ã‚’çµåˆ
    const allSummaries = detailedData.summaries
      .map(s => s.englishSummary)
      .join('\n\n');
    
    // å…¨èªå½™ã‚’é‡è¤‡æ’é™¤
    const allVocabulary = [...new Set(
      detailedData.summaries.flatMap(s => s.vocabulary || [])
    )];
    
    // GPT-5é«˜æ¨è«–ã§ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    const finalReport = await generateFinalReport(
      allTranscripts,
      allSummaries,
      allVocabulary
    );
    
    const reportTime = Date.now() - reportStartTime;
    console.log(`âœ… AI report generated in ${reportTime}ms`);
    
    // ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    detailedData.finalReport = finalReport;
    
    // AIãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    const AI_REPORT_FILE = path.join(OUT_DIR, `${TEST_NAME}_ai-report_${RUN_ID}.md`);
    fs.writeFileSync(AI_REPORT_FILE, finalReport);
    console.log(`ğŸ“„ AI Report saved: ${AI_REPORT_FILE}`);
    
  } catch (error) {
    console.error('âŒ AI report generation failed:', error.message);
    log('error', { 
      stage: 'final-report', 
      error: error.message 
    });
  }
  
  console.log('\nğŸ“Š Generating reports...');
  
  // è©³ç´°ãƒ‡ãƒ¼ã‚¿ä¿å­˜
  fs.writeFileSync(DETAILED_FILE, JSON.stringify(detailedData, null, 2));
  
  // ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜
  fs.writeFileSync(METRICS_FILE, JSON.stringify(metrics, null, 2));
  
  // Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
  generateMarkdownReport();
  
  // ãƒ­ã‚°ã‚¹ãƒˆãƒªãƒ¼ãƒ çµ‚äº†
  logStream.end();
  
  console.log('\n' + '='.repeat(80));
  console.log('âœ… Test completed!');
  console.log('='.repeat(80));
  console.log(`ğŸ“Š Detailed data: ${DETAILED_FILE}`);
  console.log(`ğŸ“ˆ Metrics: ${METRICS_FILE}`);
  console.log(`ğŸ“„ Report: ${REPORT_FILE}`);
  console.log(`ğŸ“ Raw log: ${LOG_FILE}`);
  console.log('='.repeat(80));
  
  // ä¸»è¦KPIè¡¨ç¤º
  const totalProcessed = metrics.translations.successful + metrics.translations.empty + metrics.translations.errors;
  const successRate = totalProcessed > 0 ? 
    (metrics.translations.successful / totalProcessed) * 100 : 0;
  const emptyRate = totalProcessed > 0 ? 
    (metrics.translations.empty / totalProcessed) * 100 : 0;
  const avgFirstPaint = metrics.performance.firstPaints.length > 0 ?
    metrics.performance.firstPaints.reduce((a, b) => a + b, 0) / metrics.performance.firstPaints.length : 0;
  
  console.log('\nğŸ“Š KEY RESULTS:');
  console.log(`  Total Translations: ${detailedData.stage6_display.translations.length}`);
  console.log(`  Success Rate: ${successRate.toFixed(1)}%`);
  console.log(`  Empty Rate: ${emptyRate.toFixed(1)}%`);
  console.log(`  Avg First Paint: ${avgFirstPaint.toFixed(0)}ms`);
  console.log(`  Total Cost: $${detailedData.stage4_gptProcessing.cost.toFixed(4)}`);
  console.log('='.repeat(80));
  
  process.exit(0);
}

// ========== Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ ==========
function generateMarkdownReport() {
  const duration = metrics.duration / 1000;
  const minutes = Math.floor(duration / 60);
  const seconds = Math.floor(duration % 60);
  
  let report = `# Hayes.wav Detailed Production Test Report

## Test Information
- **Test ID**: ${RUN_ID}
- **Audio File**: ${AUDIO_FILE}
- **Duration**: ${minutes}:${String(seconds).padStart(2, '0')}
- **Date**: ${new Date().toISOString()}

## 6-Stage Pipeline Performance

### â‘ éŸ³å£°å…¥åŠ› (Audio Input)
- **Total Frames**: ${detailedData.stage1_audioInput.totalFrames}
- **Total Data**: ${(detailedData.stage1_audioInput.totalBytes / 1024 / 1024).toFixed(2)} MB
- **Frame Samples**: ${detailedData.stage1_audioInput.frameSamples.length}

### â‘¡ASRå‡¦ç† (Speech Recognition)
- **Model**: ${DG_MODEL}
- **Interim Results**: ${metrics.asr.interim}
- **Final Results**: ${metrics.asr.final}
- **Avg Confidence**: ${metrics.asr.confidence.length > 0 ? 
    (metrics.asr.confidence.reduce((a, b) => a + b, 0) / metrics.asr.confidence.length * 100).toFixed(1) : 0}%

### â‘¢ç¿»è¨³è¦æ±‚ (Translation Requests)
- **Total Requests**: ${detailedData.stage3_translationRequests.requests.length}
- **Throttled**: ${detailedData.stage3_translationRequests.throttled}

### â‘£GPTå‡¦ç† (GPT Processing)
- **Model**: ${CURRENT_MODEL}
- **Total Requests**: ${detailedData.stage4_gptProcessing.requests.length}
- **Total Tokens**: ${detailedData.stage4_gptProcessing.tokens}
- **Total Cost**: $${detailedData.stage4_gptProcessing.cost.toFixed(4)}

### â‘¤å‡ºåŠ›æŠ½å‡º (Output Extraction)
- **Successful**: ${detailedData.stage5_outputExtraction.successful.length}
- **Empty**: ${detailedData.stage5_outputExtraction.empty.length}

### â‘¥è¡¨ç¤º (Display to User)
- **Total Translations Shown**: ${detailedData.stage6_display.translations.length}
- **Average First Paint**: ${metrics.performance.firstPaints.length > 0 ?
    (metrics.performance.firstPaints.reduce((a, b) => a + b, 0) / metrics.performance.firstPaints.length).toFixed(0) : 0}ms

## Timeline (åˆ†å˜ä½ã®è»¢å†™å†…å®¹)

`;

  // ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³è¿½åŠ 
  for (const entry of detailedData.stage2_asr.timeline.slice(0, 20)) {
    report += `### ${entry.timeRange}
- **Words**: ${entry.wordCount}
- **Content**: "${entry.transcript.substring(0, 200)}..."

`;
  }

  // ã‚µãƒ³ãƒ—ãƒ«ç¿»è¨³è¿½åŠ 
  report += `## Sample Translations (ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡¨ç¤ºãƒ‡ãƒ¼ã‚¿)

`;
  
  for (const trans of detailedData.stage6_display.translations.slice(0, 10)) {
    report += `### [${trans.timeString}] ${trans.isInterim ? '(Interim)' : '(Final)'}
- **Original**: "${trans.original}"
- **Translation**: "${trans.translation}"
- **First Paint**: ${trans.firstPaintMs ? trans.firstPaintMs.toFixed(0) : 'N/A'}ms
- **Total Time**: ${trans.totalTimeMs.toFixed(0)}ms
- **Model**: ${trans.model}

`;
  }
  
  report += `---
Generated: ${new Date().toISOString()}
`;
  
  fs.writeFileSync(REPORT_FILE, report);
}

// ========== ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ ==========
async function main() {
  console.log('='.repeat(80));
  console.log('Hayes.wav Detailed Production Test');
  console.log(`Test ID: ${RUN_ID}`);
  console.log('='.repeat(80));
  
  // éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
  if (!loadAudio()) {
    process.exit(1);
  }
  
  // Deepgramæ¥ç¶š
  connectDeepgram();
}

// ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
process.on('uncaughtException', (error) => {
  console.error('Uncaught exception:', error);
  log('error', { stage: 'system', error: error.message });
  process.exit(1);
});

process.on('unhandledRejection', (reason, promise) => {
  console.error('Unhandled rejection:', reason);
  log('error', { stage: 'system', error: String(reason) });
});

// å®Ÿè¡Œ
main();